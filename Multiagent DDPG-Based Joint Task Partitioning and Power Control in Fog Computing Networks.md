#摘要
霧運算有助於**緩解資源受限的行動裝置（MD）運行運算密集應用的壓力**。本文研究了包含多個 MD 和霧設備（FD）的霧計算網路中的==任務劃分和功率控制聯合問題==。其中，每個 MD 需要==在延遲和能耗約束下完成週期性計算任務==。每個任務可以根據任務劃分策略和傳輸功率策略劃分成多個子任務並卸載到 FD 上，以降低任務執行延遲和能耗。為此，我們提出了一種==**基於多智能體深度確定性策略梯度multiagent deep deterministic policy gradient（MADDPG）的 MD 任務卸載演算法，以最大化包括執行延遲和能耗在內的長期系統效用**==。每個 MD 將本地資訊（例如任務需求、可用通訊和運算資源、自身的運算資源和電池電量）輸入到分散式執行器網路中，以產生任務卸載策略；同時，集中式評論家網路用於更新執行器網路的權重，從而提升卸載效能。數值模擬結果表明，所提出的方案能夠有效提高系統效用，並降低平均執行延遲和平均能耗。
#offloading 

1）單一用戶任務卸載問題
2）多用戶任務卸載問題
後者由於用戶間的競爭或合作而更為複雜。因此，多用戶任務卸載問題的最優策略不能簡單地疊加到單一用戶模型的最優策略上。通常，多用戶任務卸載問題可以建模為多人卸載博弈[12]。然而，在多人卸載博弈中，計算複雜度可能隨著用戶和卸載節點（FD）數量的增加呈指數級增長，並且在缺乏全局資訊的情況下難以獲得全局最優策略。
基於深度強化學習（DRL）的任務卸載技術

與 SADRL 相比，MADRL 有學習環境不穩定的問題。在 MADRL 中，每個智能體不僅要考慮環境本身的動態變化，還要考慮其他智能體的行為。因此，從單一智能體的角度來看，其他智能體也是環境的一部分。當所有智能體都在學習改變策略時，環境不穩定，訓練收斂性無法保證。此外，由於多智能體系統中缺乏全局資訊共享和集中控制，每個智能體僅根據其局部資訊做出決策，而無法了解其他智能體的行為和狀態，這就需要一個有效的學習框架。
#貢獻
本文研究了具有多個 MD 和 FD 的霧運算網路中的聯合任務分割和功耗控制問題。此聯合任務劃分和功耗控制問題被視為一個連續決策問題，旨在最大化系統的長期效用，同時考慮每個 MD 的任務執行延遲和能耗。

考慮到環境的動態性以及任務分割和功率控制的高維度動作空間，我們將問題轉換為多用戶隨機卸載博弈，並基於 MADDPG 模型求解問題。在 MADDPG 模型中，每個 MD（多用戶決策者）被建模為一個智能體，僅向集中式評論家網路共享局部資訊和環境觀測數據，同時透過分散式行動者網路進行局部決策。

#結論
本文研究了具有多個 MD 和 FD 的霧計算網路中的聯合任務劃分和功率控制問題。其中，每個 MD 都受到能量約束，其計算任務對延遲非常敏感，需要在系統時隙內執行。為了滿足延遲和能耗約束，每個 MD 的任務可以被分割成子任務，並由 MD 和 FD 協同執行。為了解決這個問題，我們提出了一種基於 MADDPG 的任務卸載演算法，用於確定每個 MD 的任務劃分策略和傳輸功率策略，以最大化系統在執行延遲和能耗方面的長期效用。基於大量的模擬實驗，我們驗證了所提出的演算法在不同輸入任務規模、FD 和 MD 數量下相對於其他基線方案的有效性。在未來的工作中，我們將對此問題進行深入分析，並透過引入註意力機制來克服 MD 數量較大時訓練過程中的不穩定性。此外，一些最先進的技術，如聯邦強化學習，可以應用於降低資訊共享成本和保護隱私。

#大綱
	#優點  有效降低平均延遲與能耗
	#缺點 大規模MD不穩定 ,資訊分享成本高
	#改進
